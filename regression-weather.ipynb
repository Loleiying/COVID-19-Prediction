{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import date\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsubmit = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")\nweather = pd.read_csv(\"/kaggle/input/weather-data/training_data_with_weather_info_week_4.csv\")\ncountry = pd.read_csv(\"/kaggle/input/countryinfo/covid19countryinfo.csv\")\npopulation = pd.read_csv(\"/kaggle/input/population-sizes-worldwide/population_sizes.csv\")\nc2 = pd.read_csv(\"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\")","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\n/kaggle/input/covid19-global-forecasting-week-4/test.csv\n/kaggle/input/covid19-global-forecasting-week-4/train.csv\n/kaggle/input/population-sizes-worldwide/population_sizes.csv\n/kaggle/input/population-sizes-worldwide/sources.csv\n/kaggle/input/countryinfo/covid19countryinfo.csv\n/kaggle/input/countryinfo/covid19tests.csv\n/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv\n/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\n/kaggle/input/weather-data/__results__.html\n/kaggle/input/weather-data/__resultx__.html\n/kaggle/input/weather-data/training_data_with_weather_info_week_4.csv\n/kaggle/input/weather-data/training_data_with_weather_info_week_1.csv\n/kaggle/input/weather-data/__notebook__.ipynb\n/kaggle/input/weather-data/custom.css\n/kaggle/input/weather-data/__output__.json\n/kaggle/input/weather-data/__results___files/__results___2_1.png\n/kaggle/input/weather-data/__results___files/__results___3_0.png\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Transformation\n1.  Combine the 'Country_Region' and 'Province_State' columns into 'country_province'.\n2.  Calculate the cumulative cases and fatalities for each country_province as 'cumCases' and 'cumDeath'\n3.  Chagnge the datatype of 'Date' to datetime  \n4.  Log transform the cases and deaths\n5.  Add 'prevCases' and 'prevDeath' for train data\n6.  Add 'Days' since Jan-22-2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Steps 1 to 6\ndef dataTrans(df, purpose):\n    # 1. Combine the Country_Region and Province_State columns into country_province.\n    df.Province_State[df['Province_State'].isnull()] = '' # change the null to empty string\n    df['country_province'] = df.apply(lambda x: x.Country_Region+'-' if x.Province_State == '' else x.Country_Region+'-'+x.Province_State, axis = 1)\n    # 3. Chagnge the datatype of Date to datetime\n    df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n   \n    ########################################################################################################################################\n     # 7.  Add a column for the days since the first cases\n    df[\"Days\"] = (df.Date - pd.Timestamp('2020-01-22 00:00:00')).dt.days\n    ########################################################################################################################################\n    \n    # 4. Log transform the cases and deaths.\n    df['log_ConfirmedCases'] = np.log(df['ConfirmedCases'])\n    df['log_Fatalities'] = np.log(df['Fatalities'])\n    \n    ##################################################################################################\n    # for training dataset only\n    if (purpose == 'train'):\n        # 2. Calculate the cumulative cases and fatalities for each country_province for the train data\n        # 5.  Add column for prevCases and prevDeath\n        cumCases = pd.Series()\n        cumDeath = pd.Series()\n        prevCases = pd.Series()\n        prevDeath = pd.Series()\n        \n        for region in df.country_province.unique():\n            cases = df.log_ConfirmedCases[df.country_province==region].cumsum() # cumulative sum of log cases\n            death = df.log_Fatalities[df.country_province==region].cumsum() # cumulative sum of log deaths\n            \n            cumCases = pd.concat([cumCases,cases])\n            cumDeath = pd.concat([cumDeath,death])\n            prevCases = pd.concat([prevCases,pd.Series([0]),cases.iloc[:len(cases)-1]])\n            prevDeath = pd.concat([prevDeath,pd.Series([0]),death.iloc[:len(death)-1]])\n            \n        prevCases = prevCases.reset_index(drop=True)\n        prevDeath = prevDeath.reset_index(drop=True)\n        #print(len(cumCases), len(cumDeath), df.shape[0])\n        df_cum = pd.concat([df,cumCases,cumDeath,prevCases,prevDeath], axis=1)\n        #print(df.shape, df_cum.shape)\n        df_cum = df_cum.rename(columns={0:'cumCases', 1:'cumDeath', 2:'prevCases', 3:'prevDeath'})\n        return df_cum\n    ######################################################################################################\n    # for testing dataset\n    else:\n        return df\n\ndf_train = dataTrans(train,'train')","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n/opt/conda/lib/python3.6/site-packages/pandas/core/series.py:856: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Additional Dataset\n1. weather\n2. country info\n3. population"},{"metadata":{},"cell_type":"markdown","source":"## Additional dataset: Weather\nHere is the weather data:\nThe weather data is only available until April-11-2020\n\n- temp: Mean temperature for the day in degrees Fahrenheit to tenths.\n- max: Maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region, so this will sometimes not be the max for the calendar day.\n- min: Minimum temperature reported during the day in Fahrenheit to tenths--time of min temp report varies by country and region, so this will sometimes not be the min for the calendar day.\n- stp: Mean station pressure for the day in millibars to tenths.\n- slp: Mean sea level pressure for the day in millibars to tenths.\n- dewp: Mean dew point for the day in degrees Fahrenheit to tenths.\n- rh: relative humidity as ratio between actual vapour pressure (computed from dewpoint temperature)\n- ah: absolute humidity from the gas law of vapour calcuated from the actual vapour pressure (in pascals). (ah = mass / volume = pressure / (constant * temperature))\n- wdsp: Mean wind speed for the day in knots to tenths.\n- prcp: Total precipitation (rain and/or melted snow) reported during the day in inches and hundredths; will usually not end with the midnight observation--i.e., may include latter part of previous day. .00 indicates no measurable precipitation (includes a trace).\n- fog: Indicators (1 = yes, 0 = no/not reported) for the occurrence during the day"},{"metadata":{"trusted":true},"cell_type":"code","source":"c2.columns","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"Index(['Country_Region', 'Province_State', 'lat', 'lon', 'continent',\n       'population', 'area', 'density'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train.Country_Region.unique()), len(c2.Country_Region.unique())\nc2.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"  Country_Region Province_State      lat      lon continent  population  \\\n0    Afghanistan            NaN  33.0000  65.0000      Asia    38041754   \n1        Albania            NaN  41.1533  20.1683    Europe     2880917   \n2        Algeria            NaN  28.0339   1.6596    Africa    43053054   \n3        Andorra            NaN  42.5063   1.5218    Europe       77142   \n4         Angola            NaN -11.2027  17.8739    Africa    31825295   \n\n      area  density  \n0   652230    58.33  \n1    28748   100.21  \n2  2381741    18.08  \n3      468   164.83  \n4  1246700    25.53  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country_Region</th>\n      <th>Province_State</th>\n      <th>lat</th>\n      <th>lon</th>\n      <th>continent</th>\n      <th>population</th>\n      <th>area</th>\n      <th>density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>NaN</td>\n      <td>33.0000</td>\n      <td>65.0000</td>\n      <td>Asia</td>\n      <td>38041754</td>\n      <td>652230</td>\n      <td>58.33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Albania</td>\n      <td>NaN</td>\n      <td>41.1533</td>\n      <td>20.1683</td>\n      <td>Europe</td>\n      <td>2880917</td>\n      <td>28748</td>\n      <td>100.21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Algeria</td>\n      <td>NaN</td>\n      <td>28.0339</td>\n      <td>1.6596</td>\n      <td>Africa</td>\n      <td>43053054</td>\n      <td>2381741</td>\n      <td>18.08</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Andorra</td>\n      <td>NaN</td>\n      <td>42.5063</td>\n      <td>1.5218</td>\n      <td>Europe</td>\n      <td>77142</td>\n      <td>468</td>\n      <td>164.83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Angola</td>\n      <td>NaN</td>\n      <td>-11.2027</td>\n      <td>17.8739</td>\n      <td>Africa</td>\n      <td>31825295</td>\n      <td>1246700</td>\n      <td>25.53</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnorthamerica = ['Antigua and Barbuda','Bahamas','Barbados','Belize','Canada','Costa Rica','Cuba', 'Dominica', \\\n                'Dominican Republic','El Salvador', 'Grenada', 'Guatemala','Haiti','Honduras', 'Jamaica', \\\n                'Mexico', 'Nicaragua', 'Panama','Saint Kitts and Nevis', 'Saint Lucia','Saint Vincent and the Grenadines',\\\n                'Trinidad and Tobago', 'US']\nsouthamerica = ['Argentina','Bolivia', 'Brazil','Chile', 'Colombia', 'Ecuador','Guyana','Paraguay','Peru',\\\n                'Suriname','Uruguay', 'Venezuela']\nassert len(c2[c2.continent == \"Americas\"].Country_Region.unique()) == (len(northamerica)+len(southamerica))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c2['continent'] = c2.apply(lambda x: 'North_America' if x.Country_Region.isin northamerica else ('South_America' if x.Country_Region in southamerica else x.continent)) # \nc2.continent.unique()","execution_count":20,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"(\"'Series' object has no attribute 'Country_Region'\", 'occurred at index Country_Region')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-d6c5f96b7b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'continent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'North_America'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountry_Region\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorthamerica\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'South_America'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountry_Region\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msouthamerica\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6926\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6927\u001b[0m         )\n\u001b[0;32m-> 6928\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-d6c5f96b7b37>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'continent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'North_America'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountry_Region\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorthamerica\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'South_America'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountry_Region\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msouthamerica\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: (\"'Series' object has no attribute 'Country_Region'\", 'occurred at index Country_Region')"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean out the filler values for NAs and outliers in particular columns\nweather['ah'] = weather['ah'].apply(lambda x: np.nan if x==np.inf else x)\nweather['wdsp'] = weather['wdsp'].apply(lambda x: np.nan if x==999.9 else x)\nweather['prcp'] = weather['prcp'].apply(lambda x: np.nan if x==99.99 else x)\n\n# replace np.nan with mean\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nweather.loc[:,'temp':'prcp']=imputer.fit_transform(weather.loc[:,'temp':'prcp'])\n\n# check that there is no outliers in the weather dataset\nfor col in weather.columns:\n    if col in ['temp','min','max','stp','slp','dewp','rh', 'ah','wdsp','prcp','fog']:\n        #print(col,sorted(weather[col].unique(),reverse=True)[:5])\n        print(col,sum(weather[col].isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data: Country info\nIt currently contains:\n\n1. Population (2020)\n2. Density: The number of people who lives per square meter. (2020)\n3. Median age (2020)\n4. Urban population: the % of the population who lives in urban areas. (2020)\n5. Hospital beds per 1K people: I assume that the higher this number is, the lower the fatalities number would be. (2020, 2018)\n6. Forced quarantine policy initial date: I believe that a couple of weeks after this specific date, we can assume there would be a reduction of the infection rate. (updated on a daily basis)\n7. School closure policy initial date: Same as (6). (updated on a daily basis)\n8. Public places (bars, restaurants, movie theatres, etc.) closure policy initial date (4/3/2020)\n9. The maximum amount of people allowed in gatherings and the initial date of the policy (4/3/2020)\n10. Non-essential house leaving - initial date of the restriction (4/3/2020)\n11. Sex ratio grouped by age groups (amount of males per female). (2020)\n12. Lung disease death rate per 100k people, separated by sex. (2020)\n13. % of smokers within the population: The higher this number is, the higher the fatalities number would be. (2019)\n14. Amount of COVID detection test made per day: I collected this information for about 50 countries, missing 120 more. (3/22/2020)\n15. GDP-nominal (2019)\n16. Health expenses in international USD (2019, 2017, 2015)\n17. Health expenses divided by population (2020 - population), (2019, 2017, 2015 - health expenses)\n18. Average amount of children per woman - I find it as an important feature when it comes in interaction with density and school restriction variables. (2017)\n19. First patient detection date\n20. Total confirmed cases (4/3/2020)\n21. Total active cases (4/3/2020)\n22. New confirmed cases (4/3/2020)\n23. Total deaths (4/3/2020)\n24. New deaths (4/3/2020)\n25. Total recovered (4/3/2020)\n26. Amount of patients in critical situation (4/3/2020)\n27. Total cases / 1 million population (4/3/2020)\n28. Total deaths / 1 million population (4/3/2020)\n29. Average temperature (Celsius) measured between January and April. (2020)\n30. Average percentage of humidity measured between January and April. (2020)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Longitude and latitude from the weather data\nweather.rename(columns={'country+province':'country_province'}, inplace=True)\ndf_train = pd.merge(df_train, weather[['country_province','Lat','Long']], on='country_province', how='outer')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Population data\npopulation.Province_State[population.Province_State.isnull()] = ''\npopulation['country_province'] = population.apply(lambda x: x.Country_Region+'-' \n                                                  if x.Province_State == '' \n                                                  else x.Country_Region+'-'+x.Province_State, \n                                                  axis=1)\ndf_train = pd.merge(df_train, population[['country_province','Population']], on='country_province', how='outer')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_weather = df_train[df_train.Date <= pd.Timestamp('2020-04-11 00:00:00')]\ndf_train_weather.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the temp, min, and max of one country\ndf = df_train_weather[df_train_weather.country_province == \"Germany-\"]\nfigsize = (20,12)\ndf.temp.plot(figsize=figsize)\ndf['min'].plot(figsize=figsize)\ndf['max'].plot(figsize=figsize)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figsize = (10,7)\ndf_train_weather.prcp.plot(figsize=figsize) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_weather.wdsp.plot(figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, I would like to try the random forest regressor on dat before 2020-04-11 through cross validation. \n#weather.Date.max(), df_train.Date.max(), test.Date.max()\n\npredictors =['Days','prevCases','prevDeath','temp','stp','wdsp','prcp','fog','min','max','dewp','rh','ah','slp'] # \ntargets = ['cumCases']\n\n#from sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nmodel = RandomForestRegressor(n_estimators=100)\n#kf = model_selection.KFold(n_splits=3) # shuffle = True\npredictedCases = []\ncount = 0\nfor region in df_train_weather.country_province.unique():\n    #print(region)\n    df = (df_train_weather[df_train_weather.country_province == region]\n        .reset_index(drop=True))\n    \n    X_train = df.loc[list(range(75)),predictors] # day 0 to day 74\n    X_test = df.loc[list(range(75,81)),predictors] # day 75 to 81\n    \n    y_train = df.loc[list(range(75)),targets]\n    y_test = df.loc[list(range(75,81)),targets]\n    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n    # For training, fit() is used\n    #print(X_train)\n    #print(y_train)\n    model.fit(X_train, y_train)\n    \n    #print(X_test)\n    #print(y_test)\n    # Default metric is R2 for regression, which can be accessed by score()\n    print('Score of training model:',model.score(X_test, y_test))\n    \n    # For other metrics, we need the predictions of the model\n    y_pred = model.predict(X_test)\n    predictedCases.append(list(X_test))\n    predictedCases.append(y_pred)\n    #print('Mean Squared Error:',metrics.mean_squared_error(y_test, y_pred))\n    #print('R2:',metrics.r2_score(y_test, y_pred))\n    \n    #print(predictedCases)\n    #print(df.cumCases)\n    count += 1\n    if (count >= 5):\n        break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}